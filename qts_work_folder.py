import os
import json
import requests
import glob
import time
import signal
import threading
import argparse
from json_repair import repair_json
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

# é…ç½®
API_URL = "https://api.xiaomimimo.com/v1/chat/completions"
TOKEN = "sk-ck5t8uacuegk8iu97db8nr4tqgr0tsnnvq3lwvnte4d3nojc"

BATCH_SIZE = 5
MAX_RETRIES = 3
MAX_WORKERS = 40  # å¹¶å‘å¤„ç†çš„æ–‡ä»¶æ•°é‡
CHUNK_SIZE = 200  # æ¯ä¸ªåˆ†ç‰‡æ–‡ä»¶ä¿å­˜çš„è¯—è¯æ•°é‡

# å…¨å±€å˜é‡ï¼šç”¨äºä¼˜é›…é€€å‡º
shutdown_event = threading.Event()
progress_lock = threading.Lock()  # è¿›åº¦æ–‡ä»¶çš„çº¿ç¨‹é”
waitlist_lock = threading.Lock()  # waitlist æ–‡ä»¶çš„çº¿ç¨‹é”
file_locks = {}  # æ¯ä¸ªæ–‡ä»¶çš„ç‹¬ç«‹é”

# å…¨å±€è·¯å¾„å˜é‡ï¼ˆç”±å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ï¼‰
SOURCE_DIR = ""
TARGET_DIR = ""
PROGRESS_FILE = ""
WAITLIST_FILE = ""

def signal_handler(signum, frame):
    """å¤„ç† Ctrl+C ä¿¡å·ï¼Œè®¾ç½®åœæ­¢æ ‡å¿—"""
    if shutdown_event.is_set():
        # ç¬¬äºŒæ¬¡ Ctrl+Cï¼Œå¼ºåˆ¶é€€å‡º
        print("\n\nâŒ å¼ºåˆ¶é€€å‡ºï¼")
        os._exit(1)
    print("\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…åœ°åœæ­¢æ‰€æœ‰ä»»åŠ¡...ï¼ˆå†æŒ‰ä¸€æ¬¡ Ctrl+C å¼ºåˆ¶é€€å‡ºï¼‰")
    shutdown_event.set()

def load_progress() -> Dict:
    """åŠ è½½è¿›åº¦æ–‡ä»¶"""
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹ã€‚")
    return {}

def save_progress(progress: Dict):
    """ä¿å­˜è¿›åº¦æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with progress_lock:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

def update_file_progress(file_name: str, processed_count: int, total_count: int, status: str = "processing"):
    """æ›´æ–°å•ä¸ªæ–‡ä»¶çš„è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with progress_lock:
        progress = load_progress()
        progress[file_name] = {
            "processed_count": processed_count,
            "total_count": total_count,
            "status": status,
            "last_update": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

def get_file_lock(file_name: str) -> threading.Lock:
    """è·å–æˆ–åˆ›å»ºæ–‡ä»¶çš„ç‹¬ç«‹é”"""
    with progress_lock:
        if file_name not in file_locks:
            file_locks[file_name] = threading.Lock()
        return file_locks[file_name]

def get_system_prompt():
    """è¯»å– prompt.md ä½œä¸ºç³»ç»ŸæŒ‡ä»¤"""
    # ä»è„šæœ¬æ‰€åœ¨ç›®å½•è¯»å– prompt.md
    script_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_path = os.path.join(script_dir, "prompt.md")
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()

def process_poems_batch(poems_batch: List[Dict]):
    """æ‰§è¡Œå•æ¬¡ API è¯·æ±‚"""
    headers = {
        'Accept': 'application/json',
        'Authorization': f'Bearer {TOKEN}',
        'Content-Type': 'application/json'
    }
    
    payload = {
        "messages": [
            {
                "role": "system",
                "content": get_system_prompt()
            },
            {
                "role": "user",
                "content": json.dumps(poems_batch, ensure_ascii=False)
            }
        ],
        "model": "mimo-v2-flash",
        "temperature": 0.3,
        "top_p": 0.95,
        # "thinking": {
        #     "type": "enabled"
        # },
        "stream": False
    }
    
    # ç»™ 5 åˆ†é’Ÿè¶…æ—¶
    response = requests.post(API_URL, headers=headers, json=payload, timeout=300) 
    response.raise_for_status()
    data = response.json()
    
    content = data['choices'][0]['message']['content']
    
    # æ¸…ç† Markdown ä»£ç å—
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0].strip()
    elif "```" in content:
        content = content.split("```")[1].split("```")[0].strip()
    
    # å°è¯•è§£æ JSONï¼Œå¤±è´¥åˆ™ä½¿ç”¨ json_repair ä¿®å¤
    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"    âš ï¸ JSON è§£æå¤±è´¥: {e}ï¼Œå°è¯•ä¿®å¤...")
        repaired = repair_json(content)
        return json.loads(repaired)

def load_waitlist() -> List[Dict]:
    """åŠ è½½ waitlist æ–‡ä»¶"""
    if os.path.exists(WAITLIST_FILE):
        try:
            with open(WAITLIST_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  è¯»å– waitlist æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
    return []

def save_to_waitlist(poems: List[Dict], source_file: str):
    """å°†æœªå®Œæˆçš„è¯—è¯ä¿å­˜åˆ° waitlistï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    if not poems:
        return
    with waitlist_lock:
        waitlist = load_waitlist()
        for poem in poems:
            # æ·»åŠ æ¥æºæ–‡ä»¶ä¿¡æ¯ä¾¿äºè¿½è¸ª
            poem_entry = {
                "source_file": source_file,
                "title": poem.get("title", ""),
                "author": poem.get("author", ""),
                "paragraphs": poem.get("paragraphs", []),
                "added_time": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            waitlist.append(poem_entry)
        with open(WAITLIST_FILE, "w", encoding="utf-8") as f:
            json.dump(waitlist, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ“‹ å·²å°† {len(poems)} é¦–æœªå®Œæˆè¯—è¯æ·»åŠ åˆ° waitlist.json")

def process_batch_with_completion(batch_to_send: List[Dict], max_retries=MAX_RETRIES):
    """
    å¢é‡è¡¥å…¨æ¨¡å¼ï¼šé’ˆå¯¹ç¼ºå¤±çš„è¯—è¯è¿›è¡Œé‡å è¯·æ±‚ï¼Œç›´åˆ°è¡¥é½ 100%ã€‚
    æ‰¹é‡è¯·æ±‚å¤±è´¥åï¼Œæ”¹ä¸ºé€é¦–å•ç‹¬è°ƒç”¨ã€‚
    è¿”å›ï¼š(æˆåŠŸç»“æœåˆ—è¡¨, æœªå®Œæˆè¯—è¯åˆ—è¡¨)
    """
    all_results_dict = {} # key: paragraphs_str, value: result_obj
    
    # è¾…åŠ©å‡½æ•°ï¼šè·å–å†…å®¹çš„å”¯ä¸€æ ‡è¯†
    def get_id(p):
        return "".join(p.get("paragraphs", [])).strip()

    def try_batch_request(current_batch, max_attempts):
        """
        æ‰¹é‡è¯·æ±‚ API
        è¿”å›ï¼šå‰©ä½™æœªå¤„ç†çš„è¯—è¯åˆ—è¡¨
        """
        attempt = 0
        # åªå¤„ç†å½“å‰ä¼ å…¥çš„æ‰¹æ¬¡ï¼ˆå·²æ’é™¤å·²æˆåŠŸçš„è¯—è¯ï¼‰
        batch = [p for p in current_batch if get_id(p) not in all_results_dict]
        
        if not batch:
            print(f"    æ‰€æœ‰è¯—è¯å·²æœ‰ç»“æœï¼Œæ— éœ€è¯·æ±‚ã€‚")
            return []
        
        while attempt <= max_attempts and batch:
            try:
                if attempt > 0:
                    print(f"    æ­£åœ¨è¿›è¡Œå¢é‡é‡è¯• (ç¬¬ {attempt} æ¬¡)ï¼Œå‰©ä½™ {len(batch)} é¦–...")
                else:
                    print(f"    æ­£åœ¨æ‰¹é‡å¤„ç† {len(batch)} é¦–è¯—è¯...")
                
                results = process_poems_batch(batch)
                
                if isinstance(results, list):
                    new_count = 0
                    for r in results:
                        rid = get_id(r)
                        if rid not in all_results_dict:
                            all_results_dict[rid] = r
                            new_count += 1
                    
                    print(f"    æœ¬æ¬¡æˆåŠŸè·å– {len(results)} é¦–ï¼Œå…¶ä¸­æ–°è·å¾— {new_count} é¦–ã€‚")
                
                # åªä»å½“å‰æ‰¹æ¬¡ä¸­è®¡ç®—è¿˜ç¼ºå“ªäº›ï¼ˆé¿å…é‡å¤è¯·æ±‚å·²æœ‰ç»“æœçš„è¯—ï¼‰
                missing_batch = [p for p in batch if get_id(p) not in all_results_dict]
                
                if not missing_batch:
                    # å½“å‰æ‰¹æ¬¡å…¨éƒ¨è¡¥é½
                    return []
                
                batch = missing_batch
                attempt += 1
                if attempt <= max_attempts:
                    time.sleep(2) # å¤±è´¥åçš„çŸ­å»¶æ—¶
                    
            except Exception as e:
                print(f"    æ‰¹é‡è¯·æ±‚å‡ºé”™ (Attempt {attempt}): {e}")
                attempt += 1
                if attempt <= max_attempts:
                    time.sleep(5)
        
        return batch  # è¿”å›æœªå¤„ç†å®Œæˆçš„è¯—è¯

    def try_single_request(poem):
        """
        å•é¦–è¯—è¯è¯·æ±‚ API
        è¿”å›ï¼šæ˜¯å¦æˆåŠŸ
        """
        poem_title = poem.get('title', 'æœªçŸ¥')[:20]
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    print(f"      å•é¦–é‡è¯• (ç¬¬ {attempt} æ¬¡): {poem_title}")
                else:
                    print(f"      å•ç‹¬å¤„ç†: {poem_title}")
                
                results = process_poems_batch([poem])
                
                if isinstance(results, list) and len(results) > 0:
                    for r in results:
                        rid = get_id(r)
                        if rid not in all_results_dict:
                            all_results_dict[rid] = r
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–äº†è¿™é¦–è¯—
                    if get_id(poem) in all_results_dict:
                        print(f"      âœ“ å•ç‹¬å¤„ç†æˆåŠŸ: {poem_title}")
                        return True
                
                time.sleep(1)
                
            except Exception as e:
                print(f"      å•é¦–è¯·æ±‚å‡ºé”™ (Attempt {attempt}): {e}")
                if attempt < max_retries:
                    time.sleep(3)
        
        return False

    # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡è¯·æ±‚
    remaining = try_batch_request(batch_to_send, max_retries)
    
    # ç¬¬äºŒæ­¥ï¼šå¦‚æœæ‰¹é‡å¤±è´¥ï¼Œé€é¦–å•ç‹¬è°ƒç”¨
    if remaining:
        print(f"    ğŸ”„ æ‰¹é‡è¯·æ±‚é‡è¯• {max_retries} æ¬¡åä»æœ‰ {len(remaining)} é¦–æœªå®Œæˆï¼Œæ”¹ä¸ºé€é¦–å•ç‹¬è°ƒç”¨...")
        still_failed = []
        for poem in remaining:
            if get_id(poem) not in all_results_dict:
                success = try_single_request(poem)
                if not success:
                    still_failed.append(poem)
        remaining = still_failed

    # è¿”å›æŒ‰ç…§åŸå§‹é¡ºåºæ’åˆ—çš„ç»“æœï¼Œä»¥åŠæœªå®Œæˆçš„è¯—è¯
    final_ordered_list = []
    failed_poems = []
    for original in batch_to_send:
        oid = get_id(original)
        if oid in all_results_dict:
            final_ordered_list.append(all_results_dict[oid])
        else:
            print(f"    âš ï¸ ç»è¿‡æ‰¹é‡å’Œå•é¦–å¤„ç†ä»æ— æ³•è·å–è¯—è¯: {original.get('title', 'æœªçŸ¥')[:20]}")
            failed_poems.append(original)
            
    return final_ordered_list, failed_poems

def get_chunk_file_path(base_path: str, chunk_index: int) -> str:
    """
    æ ¹æ®åˆ†ç‰‡ç´¢å¼•ç”Ÿæˆåˆ†ç‰‡æ–‡ä»¶è·¯å¾„
    chunk_index 0: poet.song.1000.json (åŸºç¡€æ–‡ä»¶ï¼Œä¿å­˜ 0~199)
    chunk_index 1: poet.song.1000.1.json (ä¿å­˜ 200~399)
    chunk_index 2: poet.song.1000.2.json (ä¿å­˜ 400~599)
    """
    if chunk_index == 0:
        return base_path
    # ç§»é™¤ .json åç¼€ï¼Œæ·»åŠ åˆ†ç‰‡ç¼–å·
    base_without_ext = base_path[:-5]  # ç§»é™¤ ".json"
    return f"{base_without_ext}.{chunk_index}.json"




def process_single_file(file_path: str) -> bool:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œåˆ†ç‰‡ä¿å­˜ï¼‰
    è¿”å› True è¡¨ç¤ºå®Œæˆï¼ŒFalse è¡¨ç¤ºå¤±è´¥æˆ–è¢«ä¸­æ–­
    
    åˆ†ç‰‡è§„åˆ™ï¼š
    - æ¯ CHUNK_SIZE (200) é¦–è¯—è¯ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶
    - poet.song.1000.json ä¿å­˜ç¬¬ 0~199 é¦–
    - poet.song.1000.1.json ä¿å­˜ç¬¬ 200~399 é¦–
    - poet.song.1000.2.json ä¿å­˜ç¬¬ 400~599 é¦–
    """
    file_name = os.path.basename(file_path)
    target_path = os.path.join(TARGET_DIR, file_name)
    file_lock = get_file_lock(file_name)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
    if shutdown_event.is_set():
        return False
    
    with file_lock:
        # ä» progress.json è¯»å–å·²å¤„ç†æ•°é‡
        progress = load_progress()
        file_progress = progress.get(file_name, {})
        processed_count = file_progress.get("processed_count", 0)
        
        if processed_count > 0:
            print(f"ğŸ“„ [{file_name}] å‘ç°å·²æœ‰è¿›åº¦ï¼Œå·²å¤„ç† {processed_count} é¦–ï¼Œç»§ç»­å¤„ç†...")

        # è¯»å–æºæ–‡ä»¶
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                all_poems = json.load(f)
        except Exception as e:
            print(f"âŒ [{file_name}] è¯»å–æºæ–‡ä»¶å¤±è´¥: {e}")
            update_file_progress(file_name, 0, 0, "error")
            return False
            
        num_poems = len(all_poems)
        
        if processed_count >= num_poems:
            print(f"âœ… [{file_name}] å·²å…¨éƒ¨å¤„ç†å®Œæˆï¼Œè·³è¿‡ã€‚")
            update_file_progress(file_name, processed_count, num_poems, "completed")
            return True
            
        print(f"ğŸš€ [{file_name}] å¼€å§‹/ç»§ç»­å¤„ç†ï¼Œå…± {num_poems} é¦–è¯—ï¼Œå·²å¤„ç† {processed_count} é¦–ã€‚")
        update_file_progress(file_name, processed_count, num_poems, "processing")
        
        # å½“å‰åˆ†ç‰‡çš„æ•°æ®ç¼“å­˜
        current_chunk_index = processed_count // CHUNK_SIZE
        chunk_start = current_chunk_index * CHUNK_SIZE
        
        # åŠ è½½å½“å‰åˆ†ç‰‡å·²æœ‰çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        current_chunk_path = get_chunk_file_path(target_path, current_chunk_index)
        current_chunk_data = []
        if os.path.exists(current_chunk_path):
            try:
                with open(current_chunk_path, "r", encoding="utf-8") as f:
                    current_chunk_data = json.load(f)
            except Exception:
                current_chunk_data = []
        
        # ä»æ–­ç‚¹å¼€å§‹å¾ªç¯
        for i in range(processed_count, num_poems, BATCH_SIZE):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if shutdown_event.is_set():
                print(f"â¸ï¸  [{file_name}] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¿å­˜å½“å‰è¿›åº¦åé€€å‡º...")
                # ä¿å­˜å½“å‰åˆ†ç‰‡
                with open(current_chunk_path, "w", encoding="utf-8") as f:
                    json.dump(current_chunk_data, f, ensure_ascii=False, indent=4)
                update_file_progress(file_name, processed_count, num_poems, "paused")
                return False
            
            batch = all_poems[i : i + BATCH_SIZE]
            current_batch_num = i // BATCH_SIZE + 1
            total_batches = (num_poems - 1) // BATCH_SIZE + 1
            
            print(f"  ğŸ“ [{file_name}] æ­£åœ¨å¤„ç† batch {current_batch_num}/{total_batches} (ç´¢å¼• {i} åˆ° {min(i + BATCH_SIZE, num_poems)}) ...")
            
            # æå– API éœ€è¦çš„å­—æ®µ
            batch_to_send = []
            for p in batch:
                batch_to_send.append({
                    "title": p.get("title", ""),
                    "author": p.get("author", ""),
                    "paragraphs": p.get("paragraphs", [])
                })
            
            results, failed_poems = process_batch_with_completion(batch_to_send)
            
            # è‹¥æœ‰æœªå®Œæˆçš„è¯—è¯ï¼Œä¿å­˜åˆ° waitlist
            if failed_poems:
                save_to_waitlist(failed_poems, file_name)
            
            # å³ä½¿æœ‰éƒ¨åˆ†å¤±è´¥ï¼Œä¹Ÿè¦ç»§ç»­å¤„ç†æˆåŠŸçš„éƒ¨åˆ†
            if results:
                # æ·»åŠ ç»“æœåˆ°å½“å‰åˆ†ç‰‡
                current_chunk_data.extend(results)
                processed_count += len(results) + len(failed_poems)  # å¤±è´¥çš„ä¹Ÿè®¡å…¥å·²å¤„ç†ï¼Œå› ä¸ºå·²å­˜å…¥ waitlist
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªåˆ†ç‰‡
                new_chunk_index = (processed_count - 1) // CHUNK_SIZE if processed_count > 0 else 0
                
                if new_chunk_index > current_chunk_index:
                    # å½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œä¿å­˜å¹¶åˆ‡æ¢åˆ°æ–°åˆ†ç‰‡
                    # åˆ†å‰²æ•°æ®ï¼šå‰200é¦–ç»™å½“å‰åˆ†ç‰‡ï¼Œå‰©ä½™çš„ç»™æ–°åˆ†ç‰‡
                    items_for_current = CHUNK_SIZE - (len(current_chunk_data) - len(results))
                    
                    # ä¿å­˜æ»¡çš„åˆ†ç‰‡
                    with open(current_chunk_path, "w", encoding="utf-8") as f:
                        json.dump(current_chunk_data[:CHUNK_SIZE - (len(current_chunk_data) - len(results)) + items_for_current - len(results)], f, ensure_ascii=False, indent=4)
                    
                    # æ›´æ–°åˆ†ç‰‡ä¿¡æ¯
                    current_chunk_index = new_chunk_index
                    chunk_start = current_chunk_index * CHUNK_SIZE
                    current_chunk_path = get_chunk_file_path(target_path, current_chunk_index)
                    # æ–°åˆ†ç‰‡åªåŒ…å«æº¢å‡ºçš„æ•°æ®
                    current_chunk_data = current_chunk_data[CHUNK_SIZE:]
                
                # ä¿å­˜å½“å‰åˆ†ç‰‡
                with open(current_chunk_path, "w", encoding="utf-8") as f:
                    json.dump(current_chunk_data, f, ensure_ascii=False, indent=4)
                
                # æ›´æ–°è¿›åº¦æ–‡ä»¶
                update_file_progress(file_name, processed_count, num_poems, "processing")
                
                print(f"    âœ“ [{file_name}] Batch {current_batch_num} å·²ä¿å­˜åˆ° {os.path.basename(current_chunk_path)}ã€‚å½“å‰è¿›åº¦: {processed_count}/{num_poems}")
            elif failed_poems:
                # å…¨éƒ¨å¤±è´¥ä½†å·²å­˜å…¥ waitlistï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª batch
                processed_count += len(failed_poems)
                update_file_progress(file_name, processed_count, num_poems, "processing")
                print(f"    âš ï¸ [{file_name}] Batch {current_batch_num} å…¨éƒ¨å¤±è´¥å·²å­˜å…¥ waitlistï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹...")
            
            # é€‚å½“å»¶æ—¶
            time.sleep(1)
        
        print(f"âœ… [{file_name}] å¤„ç†å®Œæˆï¼å…± {processed_count} é¦–è¯—ã€‚\n")
        update_file_progress(file_name, processed_count, num_poems, "completed")
        return True


def main():
    """ä¸»å‡½æ•°ï¼šä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    global SOURCE_DIR, TARGET_DIR, PROGRESS_FILE, WAITLIST_FILE
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="è¯—è¯å¤„ç†è„šæœ¬ - ä»æŒ‡å®šè¾“å…¥æ–‡ä»¶å¤¹è¯»å– JSON æ–‡ä»¶ï¼Œå¤„ç†åä¿å­˜åˆ°æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python qts_work_folder.py -i ./input_folder -o ./output_folder
  python qts_work_folder.py --input ./è¯—è¯åŸå§‹ --output ./è¯—è¯æ¸…æ´—
  python qts_work_folder.py -i ./data -o ./result --pattern "*.json"
        """
    )
    parser.add_argument(
        "-i", "--input",
        required=True,
        help="è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ŒåŒ…å«å¾…å¤„ç†çš„ JSON æ–‡ä»¶"
    )
    parser.add_argument(
        "-o", "--output",
        required=True,
        help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¤„ç†åçš„æ–‡ä»¶å°†ä¿å­˜åœ¨æ­¤"
    )
    parser.add_argument(
        "-p", "--pattern",
        default="*.json",
        help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œé»˜è®¤ä¸º '*.json'ï¼ˆåŒ¹é…æ‰€æœ‰ JSON æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "-w", "--workers",
        type=int,
        default=MAX_WORKERS,
        help=f"å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º {MAX_WORKERS}"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€è·¯å¾„å˜é‡
    SOURCE_DIR = os.path.abspath(args.input)
    TARGET_DIR = os.path.abspath(args.output)
    PROGRESS_FILE = os.path.join(TARGET_DIR, "progress.json")
    WAITLIST_FILE = os.path.join(TARGET_DIR, "waitlist.json")
    max_workers = args.workers
    file_pattern = args.pattern
    
    # éªŒè¯è¾“å…¥ç›®å½•å­˜åœ¨
    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {SOURCE_DIR}")
        return
    
    if not os.path.isdir(SOURCE_DIR):
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {SOURCE_DIR}")
        return
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆæ”¯æŒ Ctrl+C ä¼˜é›…é€€å‡ºï¼‰
    signal.signal(signal.SIGINT, signal_handler)
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    if not os.path.exists(TARGET_DIR):
        os.makedirs(TARGET_DIR)
        
    # è·å–æ‰€æœ‰åŒ¹é…çš„ json æ–‡ä»¶
    source_pattern = os.path.join(SOURCE_DIR, file_pattern)
    files = glob.glob(source_pattern)
    
    if not files:
        print(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {source_pattern}")
        return
    
    # åŠ è½½ä¹‹å‰çš„è¿›åº¦ï¼Œä¼˜å…ˆå¤„ç†æœªå®Œæˆçš„æ–‡ä»¶
    progress = load_progress()
    
    # æ ¹æ®è¿›åº¦æ’åºï¼šå¤„ç†ä¸­/æš‚åœçš„ > æœªå¼€å§‹çš„ > å·²å®Œæˆçš„
    # ä¼˜å…ˆå¤„ç†é‚£äº›å·²ç»å¼€å§‹ä½†è¿˜æ²¡å®Œæˆçš„æ–‡ä»¶
    def file_priority(file_path):
        file_name = os.path.basename(file_path)
        if file_name not in progress:
            return 1  # æœªå¼€å§‹çš„æ¬¡ä¹‹
        status = progress[file_name].get("status", "")
        if status in ("processing", "paused", "error"):
            return 0  # å¤„ç†ä¸­/æš‚åœ/å‡ºé”™çš„ä¼˜å…ˆçº§æœ€é«˜ï¼ˆä¼˜å…ˆæ¢å¤ï¼‰
        if status == "completed":
            return 2  # å·²å®Œæˆçš„æœ€å
        return 1
    
    files.sort(key=file_priority)
    
    # è¿‡æ»¤æ‰å·²å®Œæˆçš„æ–‡ä»¶
    pending_files = []
    completed_count = 0
    for file_path in files:
        file_name = os.path.basename(file_path)
        if file_name in progress and progress[file_name].get("status") == "completed":
            completed_count += 1
        else:
            pending_files.append(file_path)
    
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {SOURCE_DIR}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {TARGET_DIR}")
    print(f"ğŸ“š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå·²å®Œæˆ {completed_count} ä¸ªï¼Œå¾…å¤„ç† {len(pending_files)} ä¸ªã€‚")
    print(f"ğŸ”§ å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"ğŸ’¾ è¿›åº¦æ–‡ä»¶: {PROGRESS_FILE}")
    print(f"ğŸ’¡ æç¤º: æŒ‰ Ctrl+C å¯ä»¥ä¼˜é›…åœ°åœæ­¢å¹¶ä¿å­˜è¿›åº¦")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    if not pending_files:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
        return
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ–‡ä»¶
    success_count = 0
    failed_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {executor.submit(process_single_file, file_path): file_path for file_path in pending_files}
        
        try:
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                file_name = os.path.basename(file_path)
                
                try:
                    result = future.result()
                    if result:
                        success_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    print(f"âŒ [{file_name}] å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    failed_count += 1
                
                # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡
                if shutdown_event.is_set():
                    print("\nâ¹ï¸  æ­£åœ¨å–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                    for f in future_to_file:
                        f.cancel()
                    break
                    
        except KeyboardInterrupt:
            print("\n\nâš ï¸  æ•è·åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
            shutdown_event.set()
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
    print(f"   âŒ å¤±è´¥/ä¸­æ–­: {failed_count} ä¸ªæ–‡ä»¶")
    if shutdown_event.is_set():
        print(f"   ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ–­ç‚¹ç»§ç»­")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")


if __name__ == "__main__":
    main()
